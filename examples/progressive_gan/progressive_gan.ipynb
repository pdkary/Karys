{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from karys.data.ImageDataLoader import ImageDataLoader\n",
    "from keras.losses import CategoricalCrossentropy, MeanSquaredError\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "from karys.layers.LearnedNoise import LearnedNoise\n",
    "from karys.layers.MinibatchDiscrimination import MinibatchDiscrimination\n",
    "from karys.layers.WeightedAdd import WeightedAdd\n",
    "from karys.layers.AdaptiveInstanceNormalization import AutoAdaptiveInstanceNormalization, AdaptiveInstanceNormalization\n",
    "from karys.models.bases import GraphableModelBlock\n",
    "from karys.models.convolutional_blocks import Conv2DBatchNormLeakyReluBlock, Conv2DNormActBlock, LayerNormActBlock\n",
    "from karys.models.passthrough_blocks import PassthroughBlock, PassthroughLeanredNoiseBlock\n",
    "# from karys.layers.MeanSquaredError import MeanSquaredError\n",
    "from karys.trainers.ProgressiveGanTrainer import ProgressiveGanTrainer\n",
    "from karys.models.progressive_discriminator import ProgressiveDiscriminator224x224, ProgressiveDiscriminator112x112, ProgressiveDiscriminator56x56, ProgressiveDiscriminator28x28, ProgressiveDiscriminator14x14, ProgressiveDiscriminator7x7, ProgressivePassthroughDescriminator\n",
    "from karys.models.progressive_generator import ProgressiveGenerator112x112, ProgressiveGenerator14x14, ProgressiveGenerator224x224, ProgressiveGenerator28x28, ProgressiveGenerator56x56, ProgressiveGenerator7x7, ProgressivePassthroughGenerator\n",
    "CUSTOM_OBJECTS={\n",
    "                'WeightedAdd': WeightedAdd,\n",
    "                'PassthroughBlock': PassthroughBlock,\n",
    "                'GraphableModelBlock': GraphableModelBlock,\n",
    "                'Conv2DNormActBlock': Conv2DNormActBlock,\n",
    "                'LayerNormActBlock': LayerNormActBlock,\n",
    "                'MinibatchDiscrimination': MinibatchDiscrimination,\n",
    "                'Conv2DBatchNormLeakyReluBlock': Conv2DBatchNormLeakyReluBlock,\n",
    "                'AutoAdaptiveInstanceNormalization': AutoAdaptiveInstanceNormalization,\n",
    "                'AdaptiveInstanceNormalization': AdaptiveInstanceNormalization,\n",
    "                'ProgressiveDiscriminator224x224': ProgressiveDiscriminator224x224,\n",
    "                'ProgressivePassthroughDescriminator': ProgressivePassthroughDescriminator,\n",
    "                'ProgressiveDiscriminator112x112': ProgressiveDiscriminator112x112,\n",
    "                'ProgressiveDiscriminator56x56': ProgressiveDiscriminator56x56,\n",
    "                'ProgressiveDiscriminator28x28': ProgressiveDiscriminator28x28,\n",
    "                'ProgressiveDiscriminator14x14': ProgressiveDiscriminator14x14,\n",
    "                'ProgressiveDiscriminator7x7': ProgressiveDiscriminator7x7,\n",
    "                'ProgressiveGenerator224x224': ProgressiveGenerator224x224,\n",
    "                'ProgressivePassthroughGenerator': ProgressivePassthroughGenerator,\n",
    "                'PassthroughLeanredNoiseBlock': PassthroughLeanredNoiseBlock,\n",
    "                'ProgressiveGenerator112x112': ProgressiveGenerator112x112,\n",
    "                'ProgressiveGenerator56x56': ProgressiveGenerator56x56,\n",
    "                'ProgressiveGenerator28x28': ProgressiveGenerator28x28,\n",
    "                'ProgressiveGenerator14x14': ProgressiveGenerator14x14,\n",
    "                'ProgressiveGenerator7x7': ProgressiveGenerator7x7,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"C:/Users/pdkar/dev/Datasets/Handwritten\"\n",
    "output_path = \"./examples/progressive_gan/test_output\"\n",
    "\n",
    "gen_path = output_path + \"/progressive_generator\"\n",
    "disc_path = output_path + \"/progressive_discriminator\"\n",
    "\n",
    "gen_optimizer = Adam(learning_rate=1e-4)\n",
    "disc_optimizer = Adam(learning_rate=1e-5)\n",
    "\n",
    "noise_size = 256\n",
    "feature_size = 256\n",
    "\n",
    "style_loss_fn = MeanSquaredError()\n",
    "gen_loss_fn = CategoricalCrossentropy()\n",
    "disc_loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "tensorboard_dir = output_path + \"/gan_board_logs\"\n",
    "tensorboard_writer = tf.summary.create_file_writer(tensorboard_dir)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models() -> Tuple[Model, Model, ProgressiveGanTrainer]:\n",
    "    if not os.path.exists(output_path + \"/architecture_diagrams\"):\n",
    "        os.mkdir(output_path + \"/architecture_diagrams\")\n",
    "    if not os.path.exists(output_path + \"/architecture_diagrams\"):\n",
    "        os.mkdir(output_path + \"/architecture_diagrams\")\n",
    "    global input_path, gen_optimizer, gen_loss_fn, disc_optimizer, disc_loss_fn, noise_size\n",
    "    data_loader = ImageDataLoader(input_path + \"/\", \".jpg\", 0.10, (28, 28))\n",
    "    num_categories = len(data_loader.label_set)\n",
    "    # try:\n",
    "    print(\"loading classifier...\")\n",
    "    # discriminator28 = load_model(disc_path, CUSTOM_OBJECTS)\n",
    "    # discriminator = discriminator28\n",
    "    # discriminator = ProgressiveDiscriminator14x14(feature_size=feature_size, category_size=num_categories)#,next_discriminator=discriminator28)\n",
    "    discriminator = ProgressiveDiscriminator28x28(feature_size=feature_size, category_size=num_categories)\n",
    "    discriminator.plot_graphable_model(output_path + '/architecture_diagrams')\n",
    "    discriminator = discriminator.build_graph()\n",
    "    discriminator.summary()\n",
    "\n",
    "    # try:\n",
    "    print(\"loading generator...\")\n",
    "    # generator28 = load_model(gen_path, CUSTOM_OBJECTS)\n",
    "    # generator = generator28\n",
    "    # generator = ProgressiveGenerator14x14(noise_size, num_categories)\n",
    "    generator = ProgressiveGenerator28x28(noise_size, num_categories)#, previous_generator=generator14)\n",
    "    # generator = ProgressiveGenerator56x56(latent_noise_size=noise_size, previous_generator=generator28)\n",
    "    generator.plot_graphable_model(output_path + '/architecture_diagrams')\n",
    "    generator = generator.build_graph()\n",
    "    generator.summary()\n",
    "    trainer = ProgressiveGanTrainer(generator, discriminator, gen_optimizer,\n",
    "                                    gen_loss_fn, disc_optimizer, disc_loss_fn,\n",
    "                                    style_loss_fn, data_loader,\n",
    "                                    tensorboard_writer)\n",
    "    return generator, discriminator, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, trains_per_test):\n",
    "    global disc_path\n",
    "    generator, discriminator, trainer = load_models()\n",
    "    if not os.path.exists(output_path + \"/disc_real\"):\n",
    "        os.mkdir(output_path + \"/disc_real\")\n",
    "    if not os.path.exists(output_path + \"/disc_gen\"):\n",
    "        os.mkdir(output_path + \"/disc_gen\")\n",
    "    if not os.path.exists(output_path + \"/generated\"):\n",
    "        os.mkdir(output_path + \"/generated\")\n",
    "    # cols=[\"epoch\", \"gen_loss\", \"style_loss\", \"disc_loss\", \"time_step\"]\n",
    "\n",
    "    # data_file = pd.read_csv(output_path + \"/datalog.csv\") if os.path.exists(output_path + \"/datalog.csv\") else pd.DataFrame(columns=cols)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # start_time = time()\n",
    "\n",
    "        if i % trains_per_test == 0 and i != 0:\n",
    "            gen_loss, style_loss, disc_loss = trainer.test(batch_size, 1)\n",
    "            disc_real_output_filename = output_path + \"/disc_real/\" + str(i) + \".jpg\"\n",
    "            disc_gen_output_filename = output_path + \"/disc_gen/\" + str(i) + \".jpg\"\n",
    "            gen_output_filename = output_path + \"/generated/\" + str(i) + \".jpg\"\n",
    "            trainer.save_progressive_disc_real_images(disc_real_output_filename)\n",
    "            trainer.save_progressive_disc_gen_images(disc_gen_output_filename)\n",
    "            trainer.save_progressive_gen_images(gen_output_filename)\n",
    "        else:\n",
    "            gen_loss, style_loss, disc_loss = trainer.train(batch_size, 6)\n",
    "    \n",
    "    discriminator.save(disc_path)\n",
    "    generator.save(gen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(2000, 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
