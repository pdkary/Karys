{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_to_path(new_path: str):\n",
    "    module_path = os.path.abspath(os.path.join(new_path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "    \n",
    "\n",
    "is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    !git clone https://github.com/pdkary/Karys.git\n",
    "    !cd Karys && git fetch && git pull\n",
    "    !cd Karys && pip install -r requirements.txt --quiet\n",
    "    add_to_path(\"Karys/\")\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    !cd ../../ && pip install -r requirements.txt --quiet\n",
    "else:\n",
    "    add_to_path(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.configs.TextDataConfig import TextDataConfig\n",
    "from data.configs.RandomDataConfig import RandomDataConfig\n",
    "from data.wrappers.RandomDataWrapper import RandomDataWrapper\n",
    "from data.wrappers.TextDataWrapper import TextDataWrapper\n",
    "\n",
    "text_config = TextDataConfig(25000,20,5,1)\n",
    "text_data_wrapper = TextDataWrapper.load_from_file('test_input/corpus.txt', text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.ModelWrapper import ModelWrapper\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE, MSLE, binary_crossentropy\n",
    "\n",
    "a = 0.08\n",
    "glayers = [Dense(1024),LeakyReLU(a),\n",
    "          Dense(1024),LeakyReLU(a),\n",
    "          Dense(1024),LeakyReLU(a),\n",
    "          Dense(1024),LeakyReLU(a),\n",
    "          Dense(np.prod(text_config.output_shape), ReLU())]\n",
    "goptimizer = Adam(learning_rate=5e-4)\n",
    "gloss = MSLE\n",
    "#should take in input shape, and give out output shape\n",
    "text_generator_model = ModelWrapper(text_config.input_shape, text_config.output_shape, glayers, goptimizer, gloss)\n",
    "text_generator_model.build()\n",
    "\n",
    "dlayers = [Dense(1024),LeakyReLU(a),\n",
    "          Dense(1024),LeakyReLU(a),\n",
    "          Dense(1024),LeakyReLU(a),\n",
    "          Dense(1024),LeakyReLU(a),\n",
    "          Dense(1), ReLU()]\n",
    "doptimizer = Adam(learning_rate=5e-4)\n",
    "\n",
    "dloss = MSLE\n",
    "text_discriminator_model = ModelWrapper(text_config.output_shape, [1], dlayers, doptimizer, dloss)\n",
    "text_discriminator_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.TextGanTrainer import TextGanTrainer\n",
    "from plotting.TrainPlotter import TrainPlotter\n",
    "\n",
    "if is_colab:\n",
    "    file_output = \"drive/MyDrive/Colab/Language/seinfeld.txt\"\n",
    "else:\n",
    "    file_output = \"./test_output/test.txt\"\n",
    "\n",
    "train_columns = [\"Gen Train Loss\",\"Disc Train Loss\", \"Gen Test Loss\", \"Disc Test Loss\"]\n",
    "loss_plot = TrainPlotter(moving_average_size=5,labels=train_columns)\n",
    "\n",
    "epochs=10000\n",
    "trains_per_test=20\n",
    "batch_size = 20\n",
    "batches_per_train = 5  \n",
    "\n",
    "text_trainer = TextGanTrainer(text_generator_model, text_discriminator_model, text_data_wrapper, text_data_wrapper)\n",
    "text_trainer.load_datasets()\n",
    "\n",
    "g_test_loss, d_test_loss = 0, 0\n",
    "for i in range(epochs):\n",
    "    loss_plot.start_epoch()\n",
    "    g_train_loss,d_train_loss = text_trainer.train(batch_size, batches_per_train)\n",
    "\n",
    "    if i % trains_per_test == 0 and i != 0:\n",
    "        g_test_loss, d_test_loss = text_trainer.test(10, 1)\n",
    "        with open(\"./test_output/test.txt\", 'w+')as f:\n",
    "            for gen_i,gen_o in zip(text_trainer.most_recent_gen_input,text_trainer.most_recent_gen_output):\n",
    "                t_ins = \"\\n\".join([text_data_wrapper.translate_sentence(i) for i in gen_i])\n",
    "                to = text_data_wrapper.translate_sentence(gen_o[0])\n",
    "                out = \"\\n\".join([\"INPUT:\",t_ins,\"OUTPUT:\",to])\n",
    "                f.write(out +\"\\n\\n\")\n",
    "\n",
    "    loss_plot.batch_update([g_train_loss,d_train_loss,g_test_loss,d_test_loss])\n",
    "    loss_plot.log_epoch()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
