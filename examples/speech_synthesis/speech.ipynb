{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_to_path(new_path: str):\n",
    "    module_path = os.path.abspath(os.path.join(new_path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    !git clone https://github.com/pdkary/Karys.git\n",
    "    !cd Karys && git fetch && git pull\n",
    "    !cd Karys && pip install -r requirements.txt --quiet\n",
    "    add_to_path(\"Karys/\")\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    !cd Karys && pip install -r requirements.txt --quiet\n",
    "else:\n",
    "    add_to_path(\"../../\")\n",
    "    !cd ../../ && pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.configs.TextDataConfig import TextDataConfig\n",
    "from data.wrappers.TextDataWrapper import TextDataWrapper\n",
    "\n",
    "if is_colab:\n",
    "    file_input = \"drive/MyDrive/Colab/Language/seinfeld_corpus.txt\"\n",
    "else:\n",
    "    file_input = \"./test_input/corpus.txt\"\n",
    "\n",
    "vocab_size = 2500\n",
    "sentence_length = 10\n",
    "output_length = 7\n",
    "\n",
    "text_config = TextDataConfig(vocab_size, sentence_length, output_length)\n",
    "text_data_wrapper = TextDataWrapper.load_from_file(file_input, text_config)\n",
    "text_data_wrapper.show_sentence_n(135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.ModelWrapper import ModelWrapper\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, ReLU, Activation, LSTM, Embedding, Softmax\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.losses import MSE, MSLE, binary_crossentropy, categorical_crossentropy\n",
    "\n",
    "LR = 5e-4\n",
    "\n",
    "## Generator\n",
    "a = 0.08\n",
    "g_layers = [\n",
    "    ##128 represents vector space size of the vocabulary\n",
    "    Embedding(text_config.vocab_size, 128, input_length=sentence_length),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(text_config.vocab_size), Softmax()]\n",
    "\n",
    "g_optimizer = RMSprop(learning_rate=LR)\n",
    "g_loss = categorical_crossentropy\n",
    "#should take in input shape, and give out output shape\n",
    "text_generator_model = ModelWrapper(text_config.input_shape, text_config.output_shape, g_layers, g_optimizer, g_loss, flatten_input=False)\n",
    "text_generator_model.build()\n",
    "\n",
    "## Discriminator\n",
    "d_layes = [\n",
    "    Dense(256), Activation('relu'),\n",
    "    Dense(256), Activation('relu'),\n",
    "    Dense(256), Activation('relu'),\n",
    "    Dense(1), Activation('sigmoid'),\n",
    "]\n",
    "\n",
    "d_optimizer = Adam(learning_rate=LR)\n",
    "d_loss = binary_crossentropy\n",
    "text_discriminator_model = ModelWrapper(text_config.label_shape,[1],d_layes,d_optimizer,d_loss)\n",
    "text_discriminator_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.TextTrainer import TextTrainer\n",
    "from plotting.TrainPlotter import TrainPlotter\n",
    "import json\n",
    "\n",
    "if is_colab:\n",
    "    file_output = \"drive/MyDrive/Colab/Language/output/seinfeld.json\"\n",
    "else:\n",
    "    file_output = \"./test_output/output.json\"\n",
    "\n",
    "train_columns = [\"Train Loss\",\"Test Loss\"]\n",
    "loss_plot = TrainPlotter(moving_average_size=100,labels=train_columns)\n",
    "\n",
    "epochs=10\n",
    "batch_size = 5\n",
    "trains_per_test=2\n",
    "batches_per_train = 10\n",
    "\n",
    "text_trainer = TextTrainer(text_generator_model, text_discriminator_model, text_data_wrapper)\n",
    "\n",
    "test_loss = 0\n",
    "for i in range(epochs):\n",
    "    loss_plot.start_epoch()\n",
    "    train_loss = text_trainer.train(batch_size, batches_per_train)\n",
    "\n",
    "    if i % trains_per_test == 0 and i != 0:\n",
    "        test_loss = text_trainer.test(5, 1)\n",
    "        ins = text_data_wrapper.translate_sentences(text_trainer.most_recent_inputs)\n",
    "        outs = text_data_wrapper.translate_sentences(text_trainer.most_recent_outputs)\n",
    "        \n",
    "        test_output_data = {i:o for (i,o) in zip(ins, outs)}\n",
    "        with open(file_output,'w+') as f:\n",
    "            json.dump(test_output_data,f)\n",
    "\n",
    "    loss_plot.batch_update([train_loss, test_loss])\n",
    "    loss_plot.log_epoch()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
