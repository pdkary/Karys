{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_to_path(new_path: str):\n",
    "    module_path = os.path.abspath(os.path.join(new_path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "    \n",
    "is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    !git clone https://github.com/pdkary/Karys.git\n",
    "    !cd Karys && git fetch && git pull\n",
    "    !cd Karys && pip install -r requirements.txt --quiet\n",
    "    add_to_path(\"Karys/\")\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    !cd ../../ && pip install -r requirements.txt --quiet\n",
    "    add_to_path(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.configs.ImageDataConfig import ImageDataConfig\n",
    "from data.configs.RandomDataConfig import RandomDataConfig\n",
    "from data.wrappers.RandomDataWrapper import RandomDataWrapper\n",
    "from data.wrappers.ImageDataWrapper import ImageDataWrapper\n",
    "\n",
    "if is_colab:\n",
    "  image_path = \"drive/MyDrive/Colab/Mons\"\n",
    "else:\n",
    "  image_path = \"./test_input\"\n",
    "\n",
    "image_config = ImageDataConfig(image_shape=(64,64,3),image_type=\".png\", preview_rows=2, preview_cols=3)\n",
    "random_config = RandomDataConfig([512], 0.0, 1.0, 5000)\n",
    "\n",
    "image_data_wrapper = ImageDataWrapper.load_from_file_with_single_label(image_path, \"REAL\", image_config, validation_percentage=0.25)\n",
    "random_data_wrapper = RandomDataWrapper(random_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ClassificationModel import ClassificationModel\n",
    "from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, MaxPooling2D, Flatten, Activation, BatchNormalization\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n",
    "\n",
    "classification_labels = [\"REAL\", \"FAKE\"]\n",
    "\n",
    "a = 0.08\n",
    "disc_layers = [\n",
    "    Conv2D(128,3), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(128,3), BatchNormalization(), LeakyReLU(a),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(512,3), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(512,3), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(512,3), BatchNormalization(), LeakyReLU(a),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(4096), Activation('relu'),\n",
    "    Dense(1000), Activation('relu'),\n",
    "    Dense(len(classification_labels)), Activation('sigmoid'),\n",
    "]\n",
    "optimizer = Adam(learning_rate=5e-4)\n",
    "loss = CategoricalCrossentropy(from_logits=True, reduction=Reduction.SUM)\n",
    "discriminator = ClassificationModel(image_config.image_shape, classification_labels, disc_layers, optimizer, loss)\n",
    "discriminator.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GenerationModel import GenerationModel\n",
    "from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, Activation, Flatten, Reshape, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "a = 0.08\n",
    "gen_layers = [\n",
    "    Dense(512), Activation('relu'),\n",
    "    Dense(4096), Activation('relu'),\n",
    "    Reshape((2,2,1024)),\n",
    "    UpSampling2D(),\n",
    "    Conv2D(1024,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(1024,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(1024,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    UpSampling2D(),\n",
    "    Conv2D(256,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(256,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(256,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    UpSampling2D(),\n",
    "    Conv2D(64,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(64,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(64,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    UpSampling2D(),\n",
    "    Conv2D(32,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(32,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(32,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    UpSampling2D(),\n",
    "    Conv2D(16,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(16,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "    Conv2D(3,3,padding=\"same\"), Activation('sigmoid'),\n",
    "]\n",
    "\n",
    "optimizer = Adam(learning_rate=5e-4)\n",
    "loss = CategoricalCrossentropy(from_logits=True, reduction=Reduction.SUM)\n",
    "generator = GenerationModel(random_config.shape,image_config.image_shape,gen_layers,optimizer,loss)\n",
    "generator.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting.TrainPlotter import TrainPlotter\n",
    "from trainers.ImageGanTrainer import ImageGanTrainer\n",
    "\n",
    "epochs=100\n",
    "trains_per_test=2\n",
    "batch_size = 6\n",
    "batches_per_loop = 3\n",
    "\n",
    "train_columns = [\"Gen Train Loss\",\"Disc Train Loss\", \"Gen Test Loss\", \"Disc Test Loss\"]\n",
    "loss_plot = TrainPlotter(moving_average_size=5,labels=train_columns)\n",
    "trainer = ImageGanTrainer(generator, discriminator, random_data_wrapper, image_data_wrapper)\n",
    "\n",
    "if is_colab:\n",
    "  image_output_path = image_path + \"/images\"\n",
    "else:\n",
    "  image_output_path = \"./test_output\"\n",
    "\n",
    "g_test_loss = 0\n",
    "d_test_loss = 0\n",
    "for i in range(epochs):\n",
    "  loss_plot.start_epoch()  \n",
    "  g_loss, d_loss = trainer.train(batch_size, batches_per_loop)\n",
    "\n",
    "  if i % trains_per_test == 0 and i != 0:\n",
    "    g_test_loss, d_test_loss = trainer.test(12,1)\n",
    "    test_output_filename = image_output_path + \"/train-\" + str(i) + \".png\"\n",
    "    image_data_wrapper.save_generated_images(test_output_filename, trainer.most_recent_gen_output)\n",
    "      \n",
    "  loss_plot.batch_update([g_loss,d_loss,g_test_loss,d_test_loss])\n",
    "  loss_plot.log_epoch()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
