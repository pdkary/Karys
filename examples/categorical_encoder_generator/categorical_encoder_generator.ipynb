{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Categorical_encoder_generator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","\n","def add_to_path(new_path: str):\n","    module_path = os.path.abspath(os.path.join(new_path))\n","    if module_path not in sys.path:\n","        sys.path.append(module_path)\n","\n","is_colab = True\n","\n","if is_colab:\n","    !git clone https://github.com/pdkary/Karys.git\n","    !cd Karys && git fetch && git pull\n","    !cd Karys && pip install -r requirements.txt --quiet\n","    add_to_path('Karys/')\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !cd Karys && pip install -r requirements.txt --quiet\n","else:\n","    add_to_path('../../')\n","    !cd ../../ && pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data.configs.ImageDataConfig import ImageDataConfig\n","from data.wrappers.ImageDataWrapper import ImageDataWrapper\n","import numpy as np\n","\n","if is_colab:\n","    base_path = \"drive/MyDrive/Colab/Seefood/Fruit\"\n","else:\n","    base_path = \"../discriminator/test_input/Fruit\"\n","\n","def map_to_range(input_arr,new_max,new_min):\n","    img_max = float(np.max(input_arr))\n","    img_min = float(np.min(input_arr))\n","    old_range = float(img_max - img_min + 1e-7)\n","    new_range = (new_max - new_min)\n","    return new_range*(input_arr - img_min)/old_range + float(new_min)\n","  \n","load_scale_function = lambda x: map_to_range(x, 1.0, -1.0)\n","\n","image_config = ImageDataConfig(image_shape=(128,128,3),image_type=\".jpg\", preview_rows=4, preview_cols=2, load_n_percent=20, load_scale_func=load_scale_function)\n","\n","data_wrapper = ImageDataWrapper.load_from_labelled_directories(base_path + '/', image_config, validation_percentage=0.1)\n","classification_labels = list(set(data_wrapper.image_labels.values()))\n","\n","##Quick stats about loaded images\n","print(\"----------LOADED IMAGE CATEGORIES----------\")\n","print(classification_labels)\n","print(\"NUM IMAGES: \",len(data_wrapper.image_labels))\n","\n","print(\"----------LOADED IMAGE STATS----------\")\n","value_count_dict = {v:0 for v in set(data_wrapper.image_labels.values())}\n","for x in data_wrapper.image_labels.values():\n","    value_count_dict[x]+=1\n","print(value_count_dict)\n","print(\"Max: \",np.max(list(value_count_dict.values())), max(value_count_dict,key=value_count_dict.get))\n","print(\"Min: \",np.min(list(value_count_dict.values())), min(value_count_dict,key=value_count_dict.get))\n","print(\"Mean: \",np.mean(list(value_count_dict.values())))\n","print(\"Median: \",np.median(list(value_count_dict.values())))\n","print(\"Deviation: \",np.std(list(value_count_dict.values())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LOAD_FROM_SAVED = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data.saved_models.SavedModelService import SavedModelService\n","from models.EncoderModel import EncoderModel\n","from models.ClassificationModel import ClassificationModel\n","from models.GenerationModel import GenerationModel\n","from models.EncodedClassificationModel import EncodedClassificationModel\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n","\n","if LOAD_FROM_SAVED:\n","  if is_colab:\n","    model_output_path = \"drive/MyDrive/Colab/Seefood/models\"\n","    model_reference_path = \"drive/MyDrive/Colab/Seefood/models/model_ref.json\"\n","  else:\n","    model_output_path = \"./test_model_output\"\n","    model_reference_path = \"../encoder/test_model_output/model_ref.json\"\n","\n","  saved_model_service = SavedModelService(model_reference_path)\n","\n","  encoder_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","  encoder_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","\n","  vectorizer_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","  vectorizer_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","\n","  classifier_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","  classifier_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","\n","  generator_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","  generator_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","\n","  encoder: EncoderModel = saved_model_service.load_model_by_name(\"FruitEncoder\", EncoderModel, encoder_optimizer, encoder_loss)\n","  label_vectorizer: EncoderModel = saved_model_service.load_model_by_name(\"FruitLabelVectorizer\", EncoderModel, vectorizer_optimizer, vectorizer_loss)\n","  classifier: ClassificationModel = saved_model_service.load_model_by_name(\"EncodedFruitClassifier\", ClassificationModel, classification_labels, [\"generated\"], classifier_optimizer, classifier_loss)\n","  generator: GenerationModel = saved_model_service.load_model_by_name(\"FruitGenerator\", GenerationModel, optimizer=generator_optimizer, loss=generator_loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models.ClassificationModel import ClassificationModel\n","from models.EncoderModel import EncoderModel\n","\n","from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, MaxPooling2D, Flatten, Activation, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n","\n","if not LOAD_FROM_SAVED:\n","  ENCODED_DIM = 512\n","  a = 0.08\n","\n","  encoder_layers = [\n","      Conv2D(64,3), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(64,3), BatchNormalization(), LeakyReLU(a),\n","      MaxPooling2D(),\n","      Conv2D(128,3), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(128,3), BatchNormalization(), LeakyReLU(a),\n","      MaxPooling2D(),\n","      Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n","      MaxPooling2D(),\n","      Conv2D(512,3), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(512,3), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(512,3), BatchNormalization(), LeakyReLU(a),\n","      MaxPooling2D(),\n","      Flatten(),\n","      Dense(ENCODED_DIM), Activation('tanh'),\n","      Dense(ENCODED_DIM), Activation('sigmoid'),\n","  ]\n","\n","  encoder_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","  encoder_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","  encoder = EncoderModel(image_config.image_shape, ENCODED_DIM, encoder_layers, encoder_optimizer, encoder_loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models.ClassificationModel import ClassificationModel\n","from models.EncoderModel import EncoderModel\n","\n","from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, MaxPooling2D, Flatten, Activation, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n","\n","if not LOAD_FROM_SAVED:\n","      vectorizer_layers = [\n","            Dense(256), Activation('tanh'),\n","            Dense(256), Activation('tanh'),\n","            Dense(256), Activation('tanh'),\n","            Dense(len(classification_labels)+1), Activation('softmax'),\n","      ]\n","\n","      vectorizer_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","      vectorizer_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","      label_vectorizer = EncoderModel([len(classifier)+1], ENCODED_DIM, vectorizer_layers, vectorizer_optimizer, vectorizer_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models.ClassificationModel import ClassificationModel\n","from models.EncoderModel import EncoderModel\n","\n","from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, MaxPooling2D, Flatten, Activation, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n","\n","if not LOAD_FROM_SAVED:\n","    classifier_layers = [\n","        Dense(4096), Activation('tanh'),\n","        Dense(4096), Activation('tanh'),\n","        Dense(len(classification_labels)+1), Activation('softmax'),\n","    ]\n","\n","    classifier_optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","    classifier_loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","    classifier = ClassificationModel([ENCODED_DIM],classification_labels, [\"generated\"], classifier_layers, classifier_optimizer, classifier_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models.GenerationModel import GenerationModel\n","from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, Activation, Flatten, Reshape, UpSampling2D, BatchNormalization, GaussianNoise\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n","from tensorflow_addons.layers import InstanceNormalization\n","\n","if not LOAD_FROM_SAVED:\n","  ENCODED_DIM = encoder.output_shape[-1]\n","  a = 0.08\n","  stddev = 0.05\n","\n","  gen_layers = [\n","      Dense(512), Activation('tanh'),\n","      Dense(512), Activation('tanh'),\n","      Dense(512), Activation('tanh'),    \n","      Dense(4096), Activation('tanh'),\n","      Reshape((2,2,1024)),\n","      UpSampling2D(),#GaussianNoise(stddev),\n","      Conv2D(512,3,padding=\"same\"),BatchNormalization(), LeakyReLU(a),\n","      Conv2D(512,3,padding=\"same\"),BatchNormalization(), LeakyReLU(a),\n","      Conv2D(512,3,padding=\"same\"),BatchNormalization(), LeakyReLU(a),\n","      UpSampling2D(),#GaussianNoise(stddev),\n","      Conv2D(256,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(256,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(256,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      UpSampling2D(),#GaussianNoise(stddev),\n","      Conv2D(128,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(128,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(128,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      UpSampling2D(),#GaussianNoise(stddev),\n","      Conv2D(64,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(64,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(64,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      UpSampling2D(),#GaussianNoise(stddev),\n","      Conv2D(32,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(32,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      Conv2D(32,3,padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n","      UpSampling2D(),#GaussianNoise(stddev),\n","      Conv2D(16,3,padding=\"same\"),BatchNormalization(),  LeakyReLU(a),\n","      Conv2D(16,3,padding=\"same\"),BatchNormalization(),  LeakyReLU(a),\n","      Conv2D(3,3,padding=\"same\"), BatchNormalization(), Activation('tanh'),\n","  ]\n","\n","  optimizer = Adam(learning_rate=4e-6)#, decay=1e-9)\n","  loss = CategoricalCrossentropy(reduction=Reduction.SUM)\n","  generator = GenerationModel(ENCODED_DIM, image_config.image_shape,gen_layers,optimizer,loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models.CategoricalEncoderGenerator import CategoricalEncoderGenerator\n","categorical_encoder_generator = CategoricalEncoderGenerator(encoder, label_vectorizer, generator, classifier)\n","categorical_encoder_generator.build()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from plotting.TrainPlotter import TrainPlotter\n","from trainers.CategoricalEncoderGeneratorTrainer import CategoricalEncoderGeneratorTrainer\n","\n","epochs=3250\n","trains_per_test=250\n","\n","train_columns = [\"Encoder Loss\", \"Label Vectorizer loss\", \"Decoder Loss\", \"Classifier Loss\", \"Test Encoder Loss\", \"Test Label Vectorizer loss\", \"Test Decoder Loss\", \"Test Classifier Loss\"]\n","loss_plot = TrainPlotter(moving_average_size=250, labels=train_columns)\n","trainer = CategoricalEncoderGeneratorTrainer(categorical_encoder_generator, data_wrapper,0.0, 1.0, 0.0, 1.0)\n","\n","if is_colab:\n","  image_output_path = \"drive/MyDrive/Colab/Seefood/output\"\n","else:\n","  image_output_path = \"./test_output\"\n","\n","e_test_loss, v_test_loss, d_test_loss, c_test_loss = 0, 0, 0\n","for i in range(epochs):\n","  loss_plot.start_epoch()  \n","  e_loss, v_loss, d_loss, c_loss = trainer.train(16, 1)\n","\n","  if i % trains_per_test == 0 and i != 0:\n","    e_test_loss, v_test_loss, d_test_loss, c_test_loss = trainer.test(16,1)\n","    gen_filename = image_output_path + \"/generated-\" + str(i) + \".jpg\"\n","    lbl_gen_filename = image_output_path + \"/generated-\" + str(i) + \".jpg\"\n","\n","    encoding_real_filename = image_output_path + \"/encoded-real-\" + str(i) + \".jpg\"\n","    encoding_label_filename = image_output_path + \"/encoded-label-\" + str(i) + \".jpg\"\n","    encoding_gen_filename = image_output_path + \"/encoded-gen-\" + str(i) + \".jpg\"\n","    encoding_label_gen_filename = image_output_path + \"/encoded-label_gen-\" + str(i) + \".jpg\"\n","\n","    classification_real_filename = image_output_path + \"/classified-real-\" + str(i) + \".jpg\"\n","    classification_label_filename = image_output_path + \"/classified-label-\" + str(i) + \".jpg\"\n","    classification_gen_filename = image_output_path + \"/classified-gen-\" + str(i) + \".jpg\"\n","    classification_label_gen_filename = image_output_path + \"/classified-label_gen-\" + str(i) + \".jpg\"\n","\n","    data_wrapper.save_generated_images(gen_filename, trainer.most_recent_generated)\n","    data_wrapper.save_generated_images(lbl_gen_filename, trainer.most_recent_label_generated)\n","\n","    data_wrapper.save_encoded_images(encoding_real_filename, trainer.most_recent_real_encoding, img_size=32)\n","    data_wrapper.save_encoded_images(classification_label_filename, trainer.most_recent_label_encodings, img_size=32)\n","    data_wrapper.save_encoded_images(encoding_gen_filename, trainer.most_recent_gen_encoding, img_size=32)\n","    data_wrapper.save_encoded_images(encoding_label_gen_filename, trainer.most_recent_label_gen_encoding, img_size=32)\n","\n","\n","    data_wrapper.save_classified_images(classification_real_filename, trainer.most_recent_real_classifications, img_size=32)\n","    data_wrapper.save_classified_images(classification_label_filename, trainer.most_recent_label_classifications, img_size=32)\n","    data_wrapper.save_classified_images(classification_gen_filename, trainer.most_recent_gen_classifications, img_size=32)\n","    data_wrapper.save_classified_images(classification_label_gen_filename, trainer.most_recent_label_gen_classifications, img_size=32)\n","      \n","  loss_plot.batch_update([e_loss, v_loss, d_loss, c_loss, e_test_loss, v_test_loss, d_test_loss, c_test_loss])\n","  loss_plot.log_epoch()"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
