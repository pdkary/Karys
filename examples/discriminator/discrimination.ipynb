{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_to_path(new_path: str):\n",
    "    module_path = os.path.abspath(os.path.join(new_path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "    \n",
    "is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    !git clone https://github.com/pdkary/Karys.git\n",
    "    !cd Karys && git fetch && git pull\n",
    "    !cd Karys && pip install -r requirements.txt --quiet\n",
    "    add_to_path(\"Karys/\")\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    !cd ../../ && pip install -r requirements.txt --quiet\n",
    "    add_to_path(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aight we doing HOT DOG / NOT HOT DOG discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.configs.ImageDataConfig import ImageDataConfig\n",
    "from data.wrappers.ImageDataWrapper import ImageDataWrapper\n",
    "\n",
    "if is_colab:\n",
    "  target_path = \"drive/MyDrive/Colab/Seefood/HotDog\"\n",
    "  noise_path = \"drive/MyDrive/Colab/Seefood/NotHotDog\"\n",
    "else:\n",
    "  target_path = \"./test_input/target_images\"\n",
    "  noise_path = \"./test_input/noise_images\"\n",
    "\n",
    "image_config = ImageDataConfig(image_shape=(64,64,3),image_type=\".jpg\", features_size=2, preview_rows=2, preview_cols=2)\n",
    "\n",
    "target_data_wrapper = ImageDataWrapper.load_from_file(target_path, image_config, validation_percentage=0.1)\n",
    "noise_data_wrapper = ImageDataWrapper.load_from_file(noise_path, image_config, validation_percentage=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We only need this next one if we are loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.saved_models.SavedModelService import SavedModelService\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, MSE, CategoricalCrossentropy\n",
    "\n",
    "\n",
    "load_from_saved_models = True\n",
    "\n",
    "if is_colab:\n",
    "  model_reference_path = \"drive/MyDrive/Colab/Seefood/models/model_ref.json\"\n",
    "else:\n",
    "  model_reference_path = \"./test_model_output/model_ref.json\"\n",
    "\n",
    "saved_model_service = SavedModelService(model_reference_path)\n",
    "\n",
    "if load_from_saved_models:\n",
    "  optimizer = Adam(learning_rate=5e-4)\n",
    "  loss = CategoricalCrossentropy(from_logits=True)\n",
    "  classifier = saved_model_service.load_model_by_name(\"SeefoodClassifier\", optimizer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ok if not loading, use this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ModelWrapper import ModelWrapper\n",
    "from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, MaxPooling2D, Flatten, Activation, BatchNormalization\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, MSE, CategoricalCrossentropy\n",
    "\n",
    "if not load_from_saved_models:\n",
    "  a = 0.08\n",
    "  disc_layers = [\n",
    "      Conv2D(64,3), LeakyReLU(a),\n",
    "      Conv2D(64,3), BatchNormalization(), LeakyReLU(a),\n",
    "      MaxPooling2D(),\n",
    "      Conv2D(128,3), LeakyReLU(a),\n",
    "      Conv2D(128,3), BatchNormalization(), LeakyReLU(a),\n",
    "      MaxPooling2D(),\n",
    "      Conv2D(256,3), LeakyReLU(a),\n",
    "      Conv2D(256,3), LeakyReLU(a),\n",
    "      Conv2D(256,3), BatchNormalization(), LeakyReLU(a),\n",
    "      MaxPooling2D(),\n",
    "      Conv2D(512,3, padding=\"same\"), LeakyReLU(a),\n",
    "      Conv2D(512,3, padding=\"same\"), LeakyReLU(a),\n",
    "      Conv2D(512,3, padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "      MaxPooling2D(),\n",
    "      Conv2D(512,3, padding=\"same\"), LeakyReLU(a),\n",
    "      Conv2D(512,3, padding=\"same\"), LeakyReLU(a),\n",
    "      Conv2D(512,3, padding=\"same\"), BatchNormalization(), LeakyReLU(a),\n",
    "      # MaxPooling2D(),\n",
    "      Flatten(),\n",
    "      Dense(4096), Activation('relu'),\n",
    "      Dense(1000), Activation('relu'),\n",
    "      Dense(image_config.features_size), Activation('softmax'),\n",
    "  ]\n",
    "  optimizer = Adam(learning_rate=5e-4)\n",
    "  loss = CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  classifier = ModelWrapper(image_config.image_shape, [image_config.features_size], disc_layers, optimizer, loss, flatten_input=False)\n",
    "  classifier.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting.TrainPlotter import TrainPlotter\n",
    "from trainers.ClassificationTrainer import ClassificationTrainer\n",
    "\n",
    "epochs=100\n",
    "trains_per_test=50\n",
    "batch_size = 25\n",
    "batches_per_loop = 1\n",
    "\n",
    "train_columns = [\"Hot Dog Train Loss\",\"Not Hot Dog Train Loss\", \"Hot Dog Test Loss\", \"Not Hot Dog Test Loss\"]\n",
    "loss_plot = TrainPlotter(moving_average_size=100,labels=train_columns)\n",
    "trainer = ClassificationTrainer(classifier, target_data_wrapper, noise_data_wrapper)\n",
    "\n",
    "if is_colab:\n",
    "  image_output_path = \"drive/MyDrive/Colab/Seefood/output\"\n",
    "else:\n",
    "  image_output_path = \"./test_output\"\n",
    "\n",
    "t_test_loss = 0\n",
    "n_test_loss = 0\n",
    "for i in range(epochs):\n",
    "  loss_plot.start_epoch()  \n",
    "  t_loss, n_loss = trainer.train(batch_size, batches_per_loop)\n",
    "\n",
    "  if i % trains_per_test == 0 and i != 0:\n",
    "    t_test_loss, n_test_loss = trainer.test(4,1)\n",
    "    test_output_filename = image_output_path + \"/train-\" + str(i) + \".jpg\"\n",
    "    target_data_wrapper.save_classified_images(test_output_filename, trainer.most_recent_target_output, trainer.most_recent_noise_output)\n",
    "      \n",
    "  loss_plot.batch_update([t_loss,n_loss,t_test_loss,n_test_loss])\n",
    "  loss_plot.log_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "  model_output_path = \"drive/MyDrive/Colab/Seefood/models\"\n",
    "else:\n",
    "  model_output_path = \"./test_model_output\"\n",
    "\n",
    "saved_model_service.save_model(\"SeefoodClassifier\",model_output_path, classifier)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
